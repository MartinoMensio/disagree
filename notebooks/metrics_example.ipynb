{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, I will show you how to use the `metrics` package in the annotations library. This library includes a wide variety of metrics commonly used to evaluate annotator (dis)agreements, as well as minimal visualisation capabilities. It has two classes: `Metrics` and `Krippendorff`. The reason I separated `Krippendorff` is because it relies on a number of costly functions upon initialisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import disagree\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use data of type string in this tutorial, but types int and float are also possible, and the library can handle missing values (i.e. `nan` and `None`).\n",
    "\n",
    "The data set in this tutorial will have 15 instances of data, annotated by 3 annotators. The possible labels will be `[\"cat\", \"dog\", \"cow\", \"ant\", None]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       a     b     c\n",
      "0   None   cat  None\n",
      "1   None  None  None\n",
      "2   None   dog   dog\n",
      "3   None   cat   cat\n",
      "4   None   cow   cow\n",
      "5    dog   cow   ant\n",
      "6    ant   ant   ant\n",
      "7    cat   cow  None\n",
      "8    dog  None   dog\n",
      "9    cat  None   cat\n",
      "10   cat  None   cat\n",
      "11   cow  None   cow\n",
      "12   cow  None   cow\n",
      "13  None  None  None\n",
      "14   cow  None   ant\n"
     ]
    }
   ],
   "source": [
    "test_annotations = {\"a\": [None, None, None, None, None, \"dog\", \"ant\", \"cat\", \"dog\", \"cat\", \"cat\", \"cow\", \"cow\", None, \"cow\"],\n",
    "                    \"b\": [\"cat\", None, \"dog\", \"cat\", \"cow\", \"cow\", \"ant\", \"cow\", None, None, None, None, None, None, None],\n",
    "                    \"c\": [None, None, \"dog\", \"cat\", \"cow\", \"ant\", \"ant\", None, \"dog\", \"cat\", \"cat\", \"cow\", \"cow\", None, \"ant\"]}\n",
    "df = pd.DataFrame(test_annotations)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will explore all of the different metrics available in the `Metrics` class. There are two types here: those that evaluate disagreements between more than two annotators, and those that evaluate disagreements between just two annotators. We will start with the former (this is just the popular Fleiss's kappa metric)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from disagree import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mets = metrics.Metrics(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fleiss kappa: 0.45\n"
     ]
    }
   ],
   "source": [
    "fleiss = mets.fleiss_kappa()\n",
    "print(\"Fleiss kappa: {:.2f}\".format(fleiss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we imported the `metrics` module from disagree, and used the `Metrics` class to set things up. All statistics (besides Krippendorff's alpha) are then called from the `Metrics` class as above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 5 simple metrics for comparing two annotators: joint probability, Cohen's kappa, Pearson correlation, Spearman correlation, and Kendall's tau correlation. The latter 3 output a tuple of the correlation and the p-value. \n",
    "\n",
    "Consider an evaluation of how often annotator \"b\" and \"c\" agree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohens = mets.cohens_kappa(ann1=\"b\", ann2=\"c\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint = mets.joint_probability(ann1=\"b\", ann2=\"c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson = mets.correlation(ann1=\"b\", ann2=\"c\", measure=\"pearson\")\n",
    "spearman = mets.correlation(ann1=\"b\", ann2=\"c\", measure=\"spearman\")\n",
    "kendall = mets.correlation(ann1=\"b\", ann2=\"c\", measure=\"kendall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen's kappa: 0.74\n",
      "Joint probability: 0.80\n",
      "Pearson's correlation: (0.7399400733959436, 0.15283808566654913)\n",
      "Spearman's correlation: (0.7105263157894738, 0.1786189192398147)\n",
      "Kendall's correlation: (0.6666666666666666, 0.11843292891667201)\n"
     ]
    }
   ],
   "source": [
    "print(\"Cohen's kappa: {:.2f}\".format(cohens))\n",
    "print(\"Joint probability: {:.2f}\".format(joint))\n",
    "print(\"Pearson's correlation: \" + str(pearson))\n",
    "print(\"Spearman's correlation: \" + str(spearman))\n",
    "print(\"Kendall's correlation: \" + str(kendall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these metrics comparing two annotators, you can visualise the metric in a matrix for all annotators by using the `metric_matrix` method. The only required argument is the function name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.   , 0.25 , 0.673],\n",
       "       [0.25 , 1.   , 0.737],\n",
       "       [0.673, 0.737, 1.   ]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mets.metric_matrix(mets.cohens_kappa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Krippendorff's alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Krippendorff's alpha follows a similar logic. This contains an option to use the `tqdm` library to output a loading bar as well, because for projects with a very large number of annotators this can take a long time, and has non-linear time complexity. (As an example, for 20,000 instances of data and 5 annotators, this takes about 10 seconds.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "kripp = metrics.Krippendorff(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of different ways to calculate Krippendorff's alpha, depending on the type of data that has been labelled. This is specified using the `data_type` argument seen below. You can use nominal, ordinal, interval, or ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = kripp.alpha(data_type=\"nominal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Krippendorff's alpha: 0.65\n"
     ]
    }
   ],
   "source": [
    "print(\"Krippendorff's alpha: {:.2f}\".format(alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

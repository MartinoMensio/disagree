{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidisagreements visualisation matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I will demonstrate how to use the `agreements` package within the annotations library, which allows for the qualitative assessment of bidisagreements (cases of data instances with 1 disagreement). The lone class here is `BiDisagreements`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from disagree import BiDisagreements\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will create a dummy dataset of labels. Remember that current capabilities allow for labels of ascending integers starting at zero, as no labels. So if you have the possible labels `[\"cat\", \"dog\", \"giraffe\", None]`, you will want to convert these to `[0, 1, 2, None]`. \n",
    "\n",
    "The data set in this tutorial will have 15 instances of data, annotated by 3 annotators. The possible labels will be `[0, 1, 2, 3, None]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_annotations = {\"a\": [None, None, None, None, None, 1, 3, 0, 1, 0, 0, 2, 2, None, 2],\n",
    "                    \"b\": [0, None, 1, 0, 2, 2, 3, 2, None, None, None, None, None, None, None],\n",
    "                    \"c\": [None, None, 1, 0, 2, 3, 3, None, 1, 0, 0, 2, 2, None, 3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(test_annotations)\n",
    "labels = [0, 1, 2, 3] # Note that you don't need to specify the presence of None labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise the instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bidis = BiDisagreements(df, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get a summary of the number of instances of data where no disagreements occurred, where 1 disagreement occurred (bidisagreement), where 2 disagreements occurred (tridisagreement), and where even more disagreements occurred:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances with:\n",
      "=========================\n",
      "No disagreement: 9\n",
      "Bidisagreement: 2\n",
      "Tridisagreement: 1\n",
      "More disagreements: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9, 2, 1, 0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bidis.agreements_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that there are 9 instances of data for which all annotators that labelled it agree. There are 2 instances whereby 2 of the annotators disagree on the label. There is 1 instance where 3 annotators disagree. There are no instances where more than 3 annotators disagree (there are only 3 annotators in this example anyway, so it would be very strange if this wasn't zero!).\n",
    "\n",
    "If you want to just have a look at the bidisagreements visually, then you can return a matrix representing the disagreements, and plot it however you like. Element $(i, j)$ is the number of bidisagreements between label $i$ and label $j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = bidis.agreements_matrix()\n",
    "mat_normalised = bidis.agreements_matrix(normalise=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 2. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [2. 0. 0. 2.]\n",
      " [0. 0. 2. 0.]]\n",
      "[[0.  0.  0.5 0. ]\n",
      " [0.  0.  0.  0. ]\n",
      " [0.5 0.  0.  0.5]\n",
      " [0.  0.  0.5 0. ]]\n"
     ]
    }
   ],
   "source": [
    "print(mat)\n",
    "print(mat_normalised)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we've seen when using the `agreements_summary` method, there were two bidisagreements. This matrix shows that 2 of these come from a disagreement between labels 2 and 0, and the other 2 come from labels 2 and 3. \n",
    "\n",
    "At this small scale, it's not very useful, but when you have 10s of thousands of labels, this can be really useful for quickly identifying where large disagreements are coming from. Once you can pinpoint where the disagreement comes from, you can go about modifying annotation schema and/or label types.\n",
    "\n",
    "Addressing these issues is essential to building datasets robust to machine learning algorithms. If your annotations are frought with disagreements, then any machine learning model will not be reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

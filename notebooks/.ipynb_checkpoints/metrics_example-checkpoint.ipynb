{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, I will show you how to use the `metrics` package in the annotations library. This library includes a wide variety of metrics commonly used to evaluate annotator (dis)agreements, as well as minimal visualisation capabilities. It has two classes: `Metrics` and `Krippendorff`. The reason I separated `Krippendorff` is because it relies on a number of costly functions upon initialisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'disagree'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-32f9cd46f54d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdisagree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'disagree'"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "import disagree\n",
    "\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use data of type string in this tutorial, but types int and float are also possible, and the library can handle missing values (i.e. NaN and NoneType).\n",
    "\n",
    "The data set in this tutorial will have 15 instances of data, annotated by 3 annotators. The possible labels will be `[\"cat\", \"dog\", \"cow\", \"ant\", None]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       a     b     c\n",
      "0   None   cat  None\n",
      "1   None  None  None\n",
      "2   None   dog   dog\n",
      "3   None   cat   cat\n",
      "4   None   cow   cow\n",
      "5    dog   cow   ant\n",
      "6    and   ant   ant\n",
      "7    cat   cow  None\n",
      "8    dog  None   dog\n",
      "9    cat  None   cat\n",
      "10   cat  None   cat\n",
      "11   cow  None   cow\n",
      "12   cow  None   cow\n",
      "13  None  None  None\n",
      "14   cow  None   ant\n"
     ]
    }
   ],
   "source": [
    "test_annotations = {\"a\": [None, None, None, None, None, \"dog\", \"and\", \"cat\", \"dog\", \"cat\", \"cat\", \"cow\", \"cow\", None, \"cow\"],\n",
    "                    \"b\": [\"cat\", None, \"dog\", \"cat\", \"cow\", \"cow\", \"ant\", \"cow\", None, None, None, None, None, None, None],\n",
    "                    \"c\": [None, None, \"dog\", \"cat\", \"cow\", \"ant\", \"ant\", None, \"dog\", \"cat\", \"cat\", \"cow\", \"cow\", None, \"ant\"]}\n",
    "df = pd.DataFrame(test_annotations)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will explore all of the different metrics available in the `Metrics` class. There are two types here: those that evaluate more than two annotators, and those that evaluate disagreements between two annotators. We will start with the former (this is just the popular Fleiss's kappa metric)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-ab7f3989066f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMetrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'labels'"
     ]
    }
   ],
   "source": [
    "mets = metrics.Metrics(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fleiss kappa: -0.29\n"
     ]
    }
   ],
   "source": [
    "fleiss = mets.fleiss_kappa()\n",
    "print(\"Fleiss kappa: {:.2f}\".format(fleiss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 5 metrics for the latter type: joint probability, Cohen's kappa, Pearson correlation, Spearman correlation, and Kendall's tau correlation. The latter 3 output a tuple of the correlation and the p-value. \n",
    "\n",
    "Consider an evaluation of how often annotator \"b\" and \"c\" agree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohens = mets.cohens_kappa(ann1=\"b\", ann2=\"c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint = mets.joint_probability(ann1=\"b\", ann2=\"c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson = mets.correlation(ann1=\"b\", ann2=\"c\", measure=\"pearson\")\n",
    "spearman = mets.correlation(ann1=\"b\", ann2=\"c\", measure=\"spearman\")\n",
    "kendall = mets.correlation(ann1=\"b\", ann2=\"c\", measure=\"kendall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen's kappa: 0.79\n",
      "Joint probability: 0.80\n",
      "Pearson's correlation: (0.9417419115948373, 0.01673155107662241)\n",
      "Spearman's correlation: (0.9210526315789475, 0.026310519685577894)\n",
      "Kendall's correlation: (0.8888888888888888, 0.037356472445581754)\n"
     ]
    }
   ],
   "source": [
    "print(\"Cohen's kappa: {:.2f}\".format(cohens))\n",
    "print(\"Joint probability: {:.2f}\".format(joint))\n",
    "print(\"Pearson's correlation: \" + str(pearson))\n",
    "print(\"Spearman's correlation: \" + str(spearman))\n",
    "print(\"Kendall's correlation: \" + str(kendall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these metrics comparing two annotators, you can visualise the metric in a matrix for all annotators by using the `metric_matrix` method. The only required argument is the function name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.   , 0.33 , 0.732],\n",
       "       [0.33 , 1.   , 0.795],\n",
       "       [0.732, 0.795, 1.   ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mets.metric_matrix(mets.cohens_kappa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Krippendorff's alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Krippendorff's alpha follows a similar logic. This uses the `tqdm` library to output a loading bar as well, because for projects with a very large number of annotators this can take a long time, and has non-linear time complexity. (As an example, for 20,000 instances of data and 5 annotators, this takes about 10 seconds.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/disagree/metrics.py:323: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  self.A = df.as_matrix().T\n",
      "100%|██████████| 4/4 [00:00<00:00, 681.31it/s]\n"
     ]
    }
   ],
   "source": [
    "kripp = metrics.Krippendorff(df, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of different ways to calculate Krippendorff's alpha, depending on the type of data that has been labelled. This is specified using the `data_type` argument seen below. You can use nominal, ordinal, interval, or ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = kripp.alpha(data_type=\"nominal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Krippendorff's alpha: 0.65\n"
     ]
    }
   ],
   "source": [
    "print(\"Krippendorff's alpha: {:.2f}\".format(alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
